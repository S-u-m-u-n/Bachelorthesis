- Find mistake in code. Why is the solution wrong if I'm using more than 32 threads per threadblock???
>>> No clue why solution is wrong
- Check assembly and compare vectorization vs without vectorization:
        cuobjdump -sass .dacecache/program/build/libmatmul.so
>>> Done, apparently no difference in Assembly... why not?

- Add cuTLASS performance for benchmarking
        Read https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
- Check in Neville's thesis how close he gets to cuBLAS
- Implement Thread Block Swizzling
- Add testcases from Neville and test performance
- Implement Split K