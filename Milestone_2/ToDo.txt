---------------------------------------------------------------------------
- Find mistake in code. Why is the solution wrong if I'm using more than 32 threads per threadblock???
>>> No clue why solution is wrong. Non-deterministic in some cases => race-condition?
>>> Need to use wcr_nonatomic=False if using more than 32 threads per threadblock!
>>> Maybe not that important, since we want to use double-buffering anyways...
- Check with cudamemcheck if there is a race condition!

>>> Compiled with -lineinfo but still no line numbers show up

Seems to be a STS / LDS problem?
https://stackoverflow.com/questions/48086305/race-condition-in-cuda-kernel in answer 1.?


                while state_dfg.entry_node():
                    if inner_schedule.map == dtypes.ScheduleType.Sequential:
                        continue


---------------------------------------------------------------------------
- Check assembly and compare vectorization vs without vectorization:
        - cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_non_vectorized.txt
        - cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_vectorized.txt
>>> Done, apparently no difference in Assembly... why not??
        => vectorization doesn't have an effect on loads from local storage! 
- Try to vectorize the global memory to shared memory loads!
- dace::vec<double, 2>

---------------------------------------------------------------------------
- Add CUTLASS performance for benchmarking. Check out https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
>>> Done.
>>> Column-Major (nn) vs. (tt) Row-Major?
>>> Using 9 results in 10 iterations... why?
>>> Warmup iterations? --warmup-iterations: at least 1. Better: 10 warmup iterations plus 100 test iterations.

=> Use --profiling-iterations=100
=> Compare Column-Major vs Row-Major!



---------------------------------------------------------------------------
- Implement Thread Block Swizzling
>>> Done.
- Do we need to check cases where the swizzled grid has a different number of blocks than the original grid, i.e. if SWIZZLE doesn't evenly divide Grid_y?
- Can check L2 performance counters with nvprof to see if swizzling thread blocks helps L2 cache
---------------------------------------------------------------------------
- Check in Neville's thesis how close he gets to cuBLAS
>>> For 1024x1024x1024 and other common cases he lies in the middle of cuBLAS and CUTLASS (see p. 55)
---------------------------------------------------------------------------
- Implement Split K
        - Check how Neville implemented it:
        >>> He created a 3-dimensional thread block grid (see p.61-62)
                Two possible implementations?
                A) Launch a kernel for each split, accumulate the result of the individual kernels
                B) Launch a single kernel, but with a third dimension of size split_k, accumulate the result along the third dimension


A short question regarding the Split K reduction: I have added a small register storage (called "accumulator") of size 1x1 where each thread stores the reduced value of the k dimension. I've checked with Neville's thesis and he doesn't seem to use local storage for the split k reduction. Compared to cuBLAS my split k reduction takes about 0.008ms on average, whereas cuBlas needs about 0.0072ms on average. Do you think the register storage is worth it like this?

---------------------------------------------------------------------------
- Add testcases from Neville and plot performance


