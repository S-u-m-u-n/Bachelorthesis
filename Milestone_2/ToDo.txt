---------------------------------------------------------------------------
- Find mistake in code. Why is the solution wrong if I'm using more than 32 threads per threadblock???
>>> No clue why solution is wrong. Non-deterministic in some cases => race-condition?



---------------------------------------------------------------------------
- Check assembly and compare vectorization vs without vectorization:
        cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_non_vectorized.txt
        cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_vectorized.txt
>>> Done, apparently no difference in Assembly... why not??



---------------------------------------------------------------------------
- Add CUTLASS performance for benchmarking. Check out https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
>>> Done.
>>> Column-Major (nn) vs. (tt) Row-Major?
>>> Using --profiling-iterations=100 results in 10 iterations... why?
>>> Warmup iterations? --warmup-iterations=1 - 10



---------------------------------------------------------------------------
- Implement Thread Block Swizzling
>>> Done.
>>> Still need to check cases where the swizzled grid has a different number of blocks than the original grid.
- Check L2 performance counters via script

---------------------------------------------------------------------------
- Check in Neville's thesis how close he gets to cuBLAS
>>> For 1024x1024x1024 and other common cases he lies in the middle between cuBLAS and CUTLASS (see page 55 in his thesis)



---------------------------------------------------------------------------
- Implement Split K



---------------------------------------------------------------------------
- Add testcases from Neville and plot performance


