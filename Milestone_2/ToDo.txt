---------------------------------------------------------------------------
- Find mistake in code. Why is the solution wrong if I'm using more than 32 threads per threadblock???
>>> No clue why solution is wrong. Non-deterministic in some cases => race-condition?
>>> Need to use wcr_nonatomic=False if using more than 32 threads per threadblock!
>>> Maybe not that important, since we want to use double-buffering anyways...
- Check with cudamemcheck if there is a race condition!

>>> K_tile_map_entry had the wrong schedule type: "dace.dtypes.ScheduleType.Sequential" instead of "dace.dtypes.ScheduleType.GPU_Device"


---------------------------------------------------------------------------
- Check assembly and compare vectorization vs without vectorization:
        - cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_non_vectorized.txt
        - cuobjdump -sass .dacecache/gemm/build/libgemm.so > assembly_vectorized.txt
>>> Done, apparently no difference in Assembly... why not??
        => vectorization doesn't have an effect on loads from local storage! 
- Try to vectorize the global memory to shared memory loads!



---------------------------------------------------------------------------
- Add CUTLASS performance for benchmarking. Check out https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
>>> Done.
>>> Column-Major (nn) vs. (tt) Row-Major?
>>> Using 9 results in 10 iterations... why?
>>> Warmup iterations? --warmup-iterations: at least 1. Better: 10 iterations.

=> Use --profiling-iterations=100
=> Compare Column-Major vs Row-Major!



---------------------------------------------------------------------------
- Implement Thread Block Swizzling
>>> Done.
- Do we need to check cases where the swizzled grid has a different number of blocks than the original grid, i.e. if SWIZZLE doesn't evenly divide Grid_y?
- Can check L2 performance counters with nvprof to see if swizzling thread blocks helps L2 cache
---------------------------------------------------------------------------
- Check in Neville's thesis how close he gets to cuBLAS
>>> For 1024x1024x1024 and other common cases he lies in the middle of cuBLAS and CUTLASS (see p. 55)
---------------------------------------------------------------------------
- Implement Split K
        - Check how Neville implemented it:
        >>> He created a 3-dimensional thread block grid (see p.61-62)
                Two possible implementations?
                A) Launch a kernel for each split, accumulate the result of the individual kernels
                B) Launch a single kernel, but with a third dimension of size split_k, accumulate the result along the third dimension



---------------------------------------------------------------------------
- Add testcases from Neville and plot performance


